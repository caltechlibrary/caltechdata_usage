{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show usage of CaltechDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (42 of 8727) |                      | Elapsed Time: 0:00:00 ETA:   0:00:21IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100% (8727 of 8727) |####################| Elapsed Time: 0:01:10 Time:  0:01:10\n"
     ]
    }
   ],
   "source": [
    "#Get metadata from CaltechDATA\n",
    "\n",
    "import os\n",
    "from ames.harvesters import get_caltechdata\n",
    "\n",
    "if os.path.isdir('data') == False:\n",
    "    os.mkdir('data')\n",
    "os.chdir('data')\n",
    "\n",
    "production = True\n",
    "collection = 'caltechdata.ds'\n",
    "\n",
    "get_caltechdata(collection,production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotnine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kk/zwcsl7yj363_wblhq73v2y1h0000gp/T/ipykernel_5813/2669341077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotnine\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Plotnine has warning - ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotnine'"
     ]
    }
   ],
   "source": [
    "#Collect submissions over time\n",
    "%matplotlib inline\n",
    "from ames.harvesters import get_records\n",
    "from py_dataset import dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as p9\n",
    "import warnings\n",
    "#Plotnine has warning - ignore\n",
    "#These will hopefully be fixed in future (v0.6?) plotnine release \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dot_paths = ['._Key','.dates','.resourceType','.subjects','.publicationYear','.contributors']\n",
    "keys = dataset.keys(collection)\n",
    "all_metadata = get_records(dot_paths,'dois',collection,keys)\n",
    "dates = []\n",
    "categories = []\n",
    "keys = []\n",
    "for record in all_metadata:\n",
    "    #Submitted date type takes precedence, then Updated date (old geo theses), then Issued\n",
    "    rdate = None\n",
    "    submitted = None\n",
    "    issued = None\n",
    "    for date in record['dates']:\n",
    "        if date['dateType']=='Submitted':\n",
    "            rdate = date['date']\n",
    "        if date['dateType']=='Updated':\n",
    "            submitted = date['date']\n",
    "        if date['dateType']=='Issued':\n",
    "            issued = date['date']\n",
    "    if rdate == None:\n",
    "        if submitted != None:\n",
    "            rdate = submitted\n",
    "        else:\n",
    "            rdate = issued\n",
    "    dates.append(rdate)\n",
    "    #We categorize records based on submission type\n",
    "    category = 'Deposit Form'\n",
    "    thesis = False\n",
    "    if 'subjects' in record:\n",
    "        for s in record['subjects']:\n",
    "            if 'TCCON' in s['subject']:\n",
    "                category = 'API'\n",
    "            if 'Bitbucket' in s['subject']:\n",
    "                category = 'API'\n",
    "            if 'Atlas of Bacterial and Archaeal Cell Structure' in s['subject']:\n",
    "                category = 'API'\n",
    "            if 'Github' in s['subject']:\n",
    "                category = 'GitHub'\n",
    "            if 'GitHub' in s['subject']:\n",
    "                category = 'GitHub'\n",
    "            if 'thesis' in s['subject']:\n",
    "                thesis=True\n",
    "    #Check if Tony handled the thesis record\n",
    "    if thesis == True:\n",
    "        if 'contributors' in record:\n",
    "            for c in record['contributors']:\n",
    "                if c['contributorName'] == 'Diaz, Tony':\n",
    "                    category = 'API'\n",
    "    categories.append(category)\n",
    "    keys.append(record['_Key'])\n",
    "data = {'dates':dates,'categories':categories,'keys':keys}\n",
    "df = pd.DataFrame(data=data)\n",
    "pd.options.display.max_rows = 2000\n",
    "df = df.astype({'dates':'datetime64'})\n",
    "#display(df.sort_values('dates'))\n",
    "#Group by year, month, and category\n",
    "df = df.groupby([df[\"dates\"].dt.year, df[\"dates\"].dt.month,df['categories']])['dates'].count()\n",
    "#Fill in categories with 0 if no values that month\n",
    "df = df.reindex(pd.MultiIndex.from_product([df.index.levels[0],df.index.levels[1],['API','Deposit Form','GitHub']]),fill_value=0)\n",
    "#Calculate cumulative sum of categories\n",
    "df = df.groupby(level=2).cumsum()\n",
    "#Pull out categories\n",
    "df = df.reset_index(level=2)\n",
    "#display(df)\n",
    "#Cut out future dates with no changes\n",
    "df = df.loc[(2017,2):(2021,5)]\n",
    "display(df.loc[(2019,4)])\n",
    "display(df.loc[(2021,4)])\n",
    "\n",
    "theme = p9.theme(axis_text_x = p9.element_text(color=\"black\", size=12,\n",
    "                                                         angle=90),\n",
    "                           axis_text_y = p9.element_text(color=\"black\", size=12))\n",
    "\n",
    "plot = p9.ggplot(df, p9.aes(x='dates.index',y='dates',group='level_2',fill='level_2')) + p9.geom_area() + theme + p9.labs(x= 'Month',y= 'Record Count',fill='Deposit Method',title=\"Deposits to CaltechDATA\")\n",
    "plot.draw()\n",
    "plot.save('data_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deposit records: 7246\n",
      "Average description length: 82.08087220535468\n",
      "Records with text or pdf: 72\n"
     ]
    }
   ],
   "source": [
    "#Look at quality of user-submitted records\n",
    "%matplotlib inline\n",
    "from ames.harvesters import get_records\n",
    "from py_dataset import dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dot_paths = ['._Key','.dates','.resourceType','.subjects','.publicationYear','.contributors','.descriptions','.electronic_location_and_access']\n",
    "keys = dataset.keys(collection)\n",
    "all_metadata = get_records(dot_paths,'dois',collection,keys)\n",
    "deposit_records = 0\n",
    "text_files = 0\n",
    "description_length = 0\n",
    "text_lengths = []\n",
    "file_sizes = []\n",
    "record_sizes = []\n",
    "for record in all_metadata:\n",
    "    #We categorize records based on submission type\n",
    "    category = 'Deposit Form'\n",
    "    thesis = False\n",
    "    if 'subjects' in record:\n",
    "        for s in record['subjects']:\n",
    "            if 'TCCON' in s['subject']:\n",
    "                category = 'API'\n",
    "            if 'Bitbucket' in s['subject']:\n",
    "                category = 'API'\n",
    "            if 'Atlas of Bacterial and Archaeal Cell Structure' in s['subject']:\n",
    "                category = 'API'\n",
    "            if 'Github' in s['subject']:\n",
    "                category = 'GitHub'\n",
    "            if 'GitHub' in s['subject']:\n",
    "                category = 'GitHub'\n",
    "            if 'thesis' in s['subject']:\n",
    "                thesis=True\n",
    "    #Check if Tony handled the thesis record\n",
    "    if thesis == True:\n",
    "        if 'contributors' in record:\n",
    "            for c in record['contributors']:\n",
    "                if c['contributorName'] == 'Diaz, Tony':\n",
    "                    category = 'API'\n",
    "    if category == 'Deposit Form':\n",
    "        deposit_records += 1\n",
    "        record_text_length = 0\n",
    "        t_file = False\n",
    "        for description in record['descriptions']:\n",
    "            #Filter out automated descriptions\n",
    "            text = description['description']\n",
    "            if 'Download Citation' not in text:\n",
    "                if 'Unique Views' not in text:\n",
    "                    description_length += len(text.split())\n",
    "                    record_text_length += len(text.split())\n",
    "        if 'electronic_location_and_access' in record:\n",
    "            record_size = 0\n",
    "            for record in record['electronic_location_and_access']:\n",
    "                name = record['electronic_name'][0]\n",
    "                if 'txt' in name:\n",
    "                    t_file = True\n",
    "                if 'pdf' in name:\n",
    "                    t_file = True\n",
    "                size = int(record['file_size'])\n",
    "                file_sizes.append(size)\n",
    "                record_size += size\n",
    "        record_sizes.append(record_size)\n",
    "        if t_file:\n",
    "            text_files += 1\n",
    "        text_lengths.append(record_text_length)\n",
    "print(f'Deposit records: {deposit_records}')\n",
    "print(f'Average description length: {description_length/deposit_records}')\n",
    "print(f'Records with text or pdf: {text_files}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'text_length':text_lengths})\n",
    "print(df.describe())\n",
    "theme = p9.theme(axis_text_x = p9.element_text(color=\"black\", size=12,\n",
    "                                                         angle=90),\n",
    "                           axis_text_y = p9.element_text(color=\"black\", size=12))\n",
    "\n",
    "p9.ggplot(df, p9.aes(x='text_length')) + p9.geom_histogram() + theme + p9.labs(x='Number of Words',y= 'Record Count',title=\"Description Length\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Storage (TB): 5.906337806145\n",
      "Average storage per record (GB): 0.0\n",
      "2018 Storage (TB): 0.155762667002\n",
      "2019 Storage (TB): 2.286860241698\n",
      "2020 Storage (TB): 1.462421418695\n",
      "2021 Storage (TB): 1.927354130248\n",
      "2019 Growth: 14.681696748737851\n",
      "2020 Growth: 0.5987094501923436\n",
      "2021 Growth: 0.49355499417178467\n",
      "count    1.192700e+04\n",
      "mean     4.952073e-01\n",
      "std      4.023197e+00\n",
      "min      3.600000e-08\n",
      "25%      4.622359e-03\n",
      "50%      4.714598e-02\n",
      "75%      4.714598e-02\n",
      "max      1.595936e+02\n",
      "Name: file_size, dtype: float64\n",
      "        record_size\n",
      "count  1.990000e+03\n",
      "mean   2.968009e+09\n",
      "std    3.166638e+10\n",
      "min    6.400000e+01\n",
      "25%    8.946240e+05\n",
      "50%    1.267686e+07\n",
      "75%    6.011785e+07\n",
      "max    1.063217e+12\n"
     ]
    }
   ],
   "source": [
    "dot_paths = ['._Key','.publicationYear','.electronic_location_and_access']\n",
    "keys = dataset.keys(collection)\n",
    "all_metadata = get_records(dot_paths,'dois',collection,keys)\n",
    "\n",
    "file_sizes = []\n",
    "record_sizes = []\n",
    "total_storage = 0\n",
    "storage_2018 = 0\n",
    "storage_2019 = 0\n",
    "storage_2020 = 0\n",
    "storage_2021 = 0\n",
    "for record in all_metadata:\n",
    "        if 'electronic_location_and_access' in record:\n",
    "            pubyear = record['publicationYear']\n",
    "            record_size = 0\n",
    "            for record in record['electronic_location_and_access']:\n",
    "                name = record['electronic_name'][0]\n",
    "                if 'txt' in name:\n",
    "                    t_file = True\n",
    "                if 'pdf' in name:\n",
    "                    t_file = True\n",
    "                size = int(record['file_size'])\n",
    "                file_sizes.append(size)\n",
    "                record_size += size\n",
    "                total_storage += size\n",
    "            record_sizes.append(record_size)\n",
    "            if pubyear == '2018':\n",
    "                storage_2018 += record_size\n",
    "            if pubyear == '2019':\n",
    "                storage_2019 += record_size\n",
    "            if pubyear == '2020':\n",
    "                storage_2020 += record_size\n",
    "            if pubyear == '2021':\n",
    "                storage_2021 += record_size\n",
    "            if t_file:\n",
    "                text_files += 1\n",
    "            text_lengths.append(record_text_length)\n",
    "print(f'Total Storage (TB): {total_storage/(10**12)}')\n",
    "print(f'Average storage per record (GB): {total_storage/len(all_metadata)//(10**9)}')\n",
    "print(f'2018 Storage (TB): {storage_2018/(10**12)}')\n",
    "print(f'2019 Storage (TB): {storage_2019/(10**12)}')\n",
    "print(f'2020 Storage (TB): {storage_2020/(10**12)}')\n",
    "print(f'2021 Storage (TB): {storage_2021/(10**12)}')\n",
    "print(f'2019 Growth: {storage_2019/storage_2018}')\n",
    "print(f'2020 Growth: {storage_2020/(storage_2018+storage_2019)}')\n",
    "print(f'2021 Growth: {storage_2021/(storage_2018+storage_2019+storage_2020)}')\n",
    "\n",
    "df2 = pd.DataFrame(data={'file_size':file_sizes})\n",
    "df2 = df2['file_size']/1000000000 #Convert to GB\n",
    "print(df2.describe())\n",
    "df3 = pd.DataFrame(data={'record_size':record_sizes})\n",
    "#df3 = df3['record_size']/1000000000 #Convert to GB\n",
    "print(df3.describe())\n",
    "#p9.ggplot(df3, p9.aes(x='record_size'))  + p9.geom_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine numbers of unique users who have submitted records and what percentage of Caltech campus has used CaltechDATA\n",
    "dot_paths = ['._Key','.dates','.owners']\n",
    "keys = dataset.keys(collection)\n",
    "metadata = get_records(dot_paths,'dois',collection,keys)\n",
    "dates = []\n",
    "owners = []\n",
    "keys = []\n",
    "for record in metadata:\n",
    "    #Submitted date type takes precedence, then Updated date (old geo theses), then Issued\n",
    "    rdate = None\n",
    "    submitted = None\n",
    "    issued = None\n",
    "    for date in record['dates']:\n",
    "        if date['dateType']=='Submitted':\n",
    "            rdate = date['date']\n",
    "        if date['dateType']=='Updated':\n",
    "            submitted = date['date']\n",
    "        if date['dateType']=='Issued':\n",
    "            issued = date['date']\n",
    "    if rdate == None:\n",
    "        if submitted != None:\n",
    "            rdate = submitted\n",
    "        else:\n",
    "            rdate = issued\n",
    "    dates.append(rdate)\n",
    "    keys.append(record['_Key'])\n",
    "    if 'owners' in record:\n",
    "        owners.append(record['owners'][0])\n",
    "    else:\n",
    "        owners.append(0)\n",
    "        #print(record['_Key'])\n",
    "data = {'dates':dates,'owners':owners,'keys':keys}    \n",
    "df = pd.DataFrame(data=data)\n",
    "pd.options.display.max_rows = 2000\n",
    "df = df.astype({'dates':'datetime64'})\n",
    "#df = df.query('20180801 < dates < 20190701')\n",
    "#display(df.sort_values('dates'))\n",
    "print('Unique users: ',df['owners'].nunique())\n",
    "#Caltech FTE from https://www.scelc.org/institutions/california-institute-technology\n",
    "print('Percentage of campus: ',100*df['owners'].nunique()/4200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine number of authors per record\n",
    "dot_paths = ['._Key','.relatedIdentifiers','.creators','.subjects']\n",
    "keys = dataset.keys(collection)\n",
    "metadata = get_records(dot_paths,'dois',collection,keys)\n",
    "num_records = len(metadata)\n",
    "total_authors = 0\n",
    "has_paper = 0\n",
    "automated_software = 0\n",
    "tccon = 0\n",
    "thesis = 0\n",
    "cell_atlas = 0\n",
    "for record in metadata:\n",
    "    total_authors += len(record['creators'])\n",
    "    if 'relatedIdentifiers' in record:\n",
    "        rel = False\n",
    "        for identifier in record['relatedIdentifiers']:\n",
    "            if identifier['relationType'] == 'IsSupplementTo':\n",
    "                rel = True \n",
    "        if rel:\n",
    "            has_paper += 1\n",
    "    if 'subjects' in record:\n",
    "        for s in record['subjects']:\n",
    "            if 'TCCON' in s['subject']:\n",
    "                tccon += 1\n",
    "            if 'Bitbucket' in s['subject']:\n",
    "                automated_software +=1\n",
    "            if 'Atlas of Bacterial and Archaeal Cell Structure' in s['subject']:\n",
    "                cell_atlas += 1\n",
    "            if 'Github' in s['subject']:\n",
    "                automated_software +=1\n",
    "            if 'GitHub' in s['subject']:\n",
    "                automated_software +=1\n",
    "            if 'thesis' in s['subject']:\n",
    "                thesis+=1\n",
    "print(f'Large record categories {thesis} Theses Records {automated_software} Software Integrations {tccon} TCCON {cell_atlas} Cell Atlas')\n",
    "print(f'Total number of authors (incl duplicates): {total_authors}')\n",
    "print(f'Number of records with publication in metadata: {has_paper}')\n",
    "print(f'Average authors per record: {total_authors/num_records}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tdict = {}\n",
    "text = ''\n",
    "for record in all_metadata:\n",
    "    if record['resourceType']['resourceTypeGeneral']=='Dataset':\n",
    "        keep=True\n",
    "        for s in record['subjects']:\n",
    "            if 'TCCON' in s['subject']:\n",
    "                keep=False\n",
    "        if keep==True:\n",
    "            for s in record['subjects']:\n",
    "                subject = s['subject']\n",
    "                text = text + ' ' + subject\n",
    "                if subject in tdict:\n",
    "                    tdict[subject] +=1\n",
    "                else:\n",
    "                    tdict[subject] = 1\n",
    "\n",
    "print(sorted(tdict.items(), key=lambda kv: kv[1]))\n",
    "wordcloud = WordCloud(min_font_size=10, max_words=100, background_color=\"white\").generate(text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
